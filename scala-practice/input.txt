
In the Spark shell, a special interpreter-aware SparkContext 
is already created for you, in the variable called sc. 
Making your own SparkContext will not work. You can set which master 
the context connects to using the --master argument, and you can add JARs to 
the classpath by passing a comma-separated list to the --jars argument. 
You can also add dependencies (e.g. Spark Packages) to your shell session 
by supplying a comma-separated list of maven coordinates to the --packages argument. 
Any additional repositories where dependencies might exist (e.g. Sonatype) can be passed to 
the --repositories argument. 
For example, to run bin/spark-shell on exactly four cores, use: